{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'email_word-prediction.ipynb',\n",
       " 'krishan_sent_outlook.CSV']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,re\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3641"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_lines = []\n",
    "\n",
    "\n",
    "include = False\n",
    "for line in open('krishan_sent_outlook.CSV','r'):\n",
    "    if line.startswith('From: Krishan'):\n",
    "        include=True\n",
    "        continue\n",
    "    elif line.startswith('From:') or line.startswith('Thanks and Regards'):\n",
    "        include = False\n",
    "        continue\n",
    "    if include:\n",
    "        if re.match(r'\\w',line) and not re.match(r'.+:',line) and len(line.split())>=3 and re.match(r'[a-zA-Z]',line):\n",
    "            valid_lines.append(line.strip().lower())\n",
    "\n",
    "valid_lines = [ re.sub('[^A-Za-z0-9]+', ' ', line).strip() for line in valid_lines ]\n",
    "valid_lines[-100:]\n",
    "len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(1000)\n",
    "tokenizer.fit_on_texts(valid_lines)\n",
    "\n",
    "def partitionlines(lines,maxlen):\n",
    "    data_x=[]\n",
    "    data_y=[]\n",
    "    for line in lines:\n",
    "        words = line\n",
    "        for i in range (0,maxlen):\n",
    "            if i >=len(words)-1:\n",
    "                break\n",
    "            data_x.append(words[0:i+1])\n",
    "            data_y.append(words[i+1])\n",
    "        for i in range (maxlen,len(words)):\n",
    "            if i >=len(words)-1:\n",
    "                break\n",
    "            data_x.append(words[i-maxlen+1:i+1])\n",
    "            data_y.append(words[i+1])\n",
    "    return data_x,data_y\n",
    "#partitionlines(['hello world every one . sunny day'],4)\n",
    "\n",
    "maxlen=5\n",
    "lines = tokenizer.texts_to_sequences(valid_lines)\n",
    "data_x,data_y = partitionlines(lines,maxlen)\n",
    "len(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train[2891],np.argmax(y_train[2891]))\n",
    "#print(x_train_shuffled[0],np.argmax(y_train_shuffled[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32283, 1000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = sequence.pad_sequences(data_x,maxlen=maxlen)\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(data_y)\n",
    "\n",
    "data_y[0]\n",
    "np.argmax(y_train[5])\n",
    "len(y_train[5])\n",
    "max(data_y)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25292, 16463,  7883, ...,  3105,   342, 16743])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.max(y_train,axis=1)\n",
    "#shuffle\n",
    "indices = np.arange(0,len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train_shuffled = x_train[indices]\n",
    "y_train_shuffled= y_train[indices]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "x_test = x_train_shuffled[-1000:]\n",
    "x_train_partial = x_train_shuffled[:-1000]\n",
    "\n",
    "y_test = y_train_shuffled[-1000:]\n",
    "y_train_partial = y_train_shuffled[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "def getmodel():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(1000, 32))\n",
    "    model.add(layers.LSTM(32))\n",
    "    model.add(layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss=losses.categorical_crossentropy,metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              33000     \n",
      "=================================================================\n",
      "Total params: 73,320\n",
      "Trainable params: 73,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25026 samples, validate on 6257 samples\n",
      "Epoch 1/100\n",
      "25026/25026 [==============================] - 13s 501us/step - loss: 6.1051 - categorical_accuracy: 0.0358 - val_loss: 5.8886 - val_categorical_accuracy: 0.0408\n",
      "Epoch 2/100\n",
      "25026/25026 [==============================] - 12s 471us/step - loss: 5.7057 - categorical_accuracy: 0.0431 - val_loss: 5.5349 - val_categorical_accuracy: 0.0564\n",
      "Epoch 3/100\n",
      "25026/25026 [==============================] - 13s 501us/step - loss: 5.3523 - categorical_accuracy: 0.0729 - val_loss: 5.2379 - val_categorical_accuracy: 0.0901\n",
      "Epoch 4/100\n",
      "25026/25026 [==============================] - 12s 461us/step - loss: 5.0575 - categorical_accuracy: 0.1053 - val_loss: 4.9863 - val_categorical_accuracy: 0.1152\n",
      "Epoch 5/100\n",
      "25026/25026 [==============================] - 13s 512us/step - loss: 4.7854 - categorical_accuracy: 0.1370 - val_loss: 4.7545 - val_categorical_accuracy: 0.1442\n",
      "Epoch 6/100\n",
      "25026/25026 [==============================] - 11s 455us/step - loss: 4.5280 - categorical_accuracy: 0.1713 - val_loss: 4.5271 - val_categorical_accuracy: 0.1768\n",
      "Epoch 7/100\n",
      "25026/25026 [==============================] - 11s 443us/step - loss: 4.2734 - categorical_accuracy: 0.2073 - val_loss: 4.3016 - val_categorical_accuracy: 0.2079\n",
      "Epoch 8/100\n",
      "25026/25026 [==============================] - 11s 458us/step - loss: 4.0346 - categorical_accuracy: 0.2410 - val_loss: 4.1028 - val_categorical_accuracy: 0.2401\n",
      "Epoch 9/100\n",
      "25026/25026 [==============================] - 11s 457us/step - loss: 3.8130 - categorical_accuracy: 0.2803 - val_loss: 3.9109 - val_categorical_accuracy: 0.2787\n",
      "Epoch 10/100\n",
      "25026/25026 [==============================] - 12s 471us/step - loss: 3.6054 - categorical_accuracy: 0.3154 - val_loss: 3.7418 - val_categorical_accuracy: 0.3073\n",
      "Epoch 11/100\n",
      "25026/25026 [==============================] - 12s 460us/step - loss: 3.4161 - categorical_accuracy: 0.3497 - val_loss: 3.5872 - val_categorical_accuracy: 0.3399\n",
      "Epoch 12/100\n",
      "25026/25026 [==============================] - 10s 411us/step - loss: 3.2431 - categorical_accuracy: 0.3782 - val_loss: 3.4397 - val_categorical_accuracy: 0.3687\n",
      "Epoch 13/100\n",
      "25026/25026 [==============================] - 11s 440us/step - loss: 3.0822 - categorical_accuracy: 0.4085 - val_loss: 3.3073 - val_categorical_accuracy: 0.3914\n",
      "Epoch 14/100\n",
      "25026/25026 [==============================] - 14s 557us/step - loss: 2.9350 - categorical_accuracy: 0.4351 - val_loss: 3.1952 - val_categorical_accuracy: 0.4135loss: 2.9366 - categorical_accuracy\n",
      "Epoch 15/100\n",
      "25026/25026 [==============================] - 8s 304us/step - loss: 2.8011 - categorical_accuracy: 0.4581 - val_loss: 3.0827 - val_categorical_accuracy: 0.4331\n",
      "Epoch 16/100\n",
      "25026/25026 [==============================] - 13s 530us/step - loss: 2.6746 - categorical_accuracy: 0.4834 - val_loss: 2.9820 - val_categorical_accuracy: 0.4456\n",
      "Epoch 17/100\n",
      "25026/25026 [==============================] - 17s 693us/step - loss: 2.5586 - categorical_accuracy: 0.5020 - val_loss: 2.8931 - val_categorical_accuracy: 0.4692\n",
      "Epoch 18/100\n",
      "25026/25026 [==============================] - 15s 596us/step - loss: 2.4543 - categorical_accuracy: 0.5212 - val_loss: 2.8039 - val_categorical_accuracy: 0.4785\n",
      "Epoch 19/100\n",
      "25026/25026 [==============================] - 14s 540us/step - loss: 2.3582 - categorical_accuracy: 0.5385 - val_loss: 2.7390 - val_categorical_accuracy: 0.4967\n",
      "Epoch 20/100\n",
      "25026/25026 [==============================] - 14s 571us/step - loss: 2.2709 - categorical_accuracy: 0.5543 - val_loss: 2.6711 - val_categorical_accuracy: 0.5121\n",
      "Epoch 21/100\n",
      "25026/25026 [==============================] - 13s 534us/step - loss: 2.1902 - categorical_accuracy: 0.5691 - val_loss: 2.6067 - val_categorical_accuracy: 0.5245\n",
      "Epoch 22/100\n",
      "25026/25026 [==============================] - 14s 566us/step - loss: 2.1148 - categorical_accuracy: 0.5836 - val_loss: 2.5538 - val_categorical_accuracy: 0.5359\n",
      "Epoch 23/100\n",
      "25026/25026 [==============================] - 13s 539us/step - loss: 2.0476 - categorical_accuracy: 0.5969 - val_loss: 2.4972 - val_categorical_accuracy: 0.5477\n",
      "Epoch 24/100\n",
      "25026/25026 [==============================] - 13s 506us/step - loss: 1.9796 - categorical_accuracy: 0.6074 - val_loss: 2.4456 - val_categorical_accuracy: 0.5555\n",
      "Epoch 25/100\n",
      "25026/25026 [==============================] - 12s 493us/step - loss: 1.9187 - categorical_accuracy: 0.6182 - val_loss: 2.4038 - val_categorical_accuracy: 0.5686\n",
      "Epoch 26/100\n",
      "25026/25026 [==============================] - 12s 487us/step - loss: 1.8613 - categorical_accuracy: 0.6298 - val_loss: 2.3546 - val_categorical_accuracy: 0.5786categorical_accuracy: 0.64 - ETA: 25s - loss: 1.8214 - categor - ETA: 8s - loss: 1 - ETA: 1s - loss: 1.8530 - ca\n",
      "Epoch 27/100\n",
      "25026/25026 [==============================] - 13s 532us/step - loss: 1.8086 - categorical_accuracy: 0.6387 - val_loss: 2.3200 - val_categorical_accuracy: 0.5854\n",
      "Epoch 28/100\n",
      "25026/25026 [==============================] - 12s 469us/step - loss: 1.7577 - categorical_accuracy: 0.6493 - val_loss: 2.3025 - val_categorical_accuracy: 0.5881\n",
      "Epoch 29/100\n",
      "25026/25026 [==============================] - 12s 467us/step - loss: 1.7138 - categorical_accuracy: 0.6552 - val_loss: 2.2542 - val_categorical_accuracy: 0.5974\n",
      "Epoch 30/100\n",
      "25026/25026 [==============================] - 13s 502us/step - loss: 1.6657 - categorical_accuracy: 0.6658 - val_loss: 2.2326 - val_categorical_accuracy: 0.6040\n",
      "Epoch 31/100\n",
      "25026/25026 [==============================] - 12s 485us/step - loss: 1.6260 - categorical_accuracy: 0.6715 - val_loss: 2.2012 - val_categorical_accuracy: 0.6072\n",
      "Epoch 32/100\n",
      "25026/25026 [==============================] - 12s 465us/step - loss: 1.5859 - categorical_accuracy: 0.6781 - val_loss: 2.1718 - val_categorical_accuracy: 0.6169\n",
      "Epoch 33/100\n",
      "25026/25026 [==============================] - 11s 456us/step - loss: 1.5478 - categorical_accuracy: 0.6855 - val_loss: 2.1507 - val_categorical_accuracy: 0.6241\n",
      "Epoch 34/100\n",
      "25026/25026 [==============================] - 12s 472us/step - loss: 1.5098 - categorical_accuracy: 0.6913 - val_loss: 2.1175 - val_categorical_accuracy: 0.6267\n",
      "Epoch 35/100\n",
      "25026/25026 [==============================] - 11s 450us/step - loss: 1.4748 - categorical_accuracy: 0.6976 - val_loss: 2.1019 - val_categorical_accuracy: 0.6337\n",
      "Epoch 36/100\n",
      "25026/25026 [==============================] - 11s 451us/step - loss: 1.4427 - categorical_accuracy: 0.7047 - val_loss: 2.0729 - val_categorical_accuracy: 0.6390\n",
      "Epoch 37/100\n",
      "25026/25026 [==============================] - 12s 464us/step - loss: 1.4117 - categorical_accuracy: 0.7098 - val_loss: 2.0574 - val_categorical_accuracy: 0.6434\n",
      "Epoch 38/100\n",
      "25026/25026 [==============================] - 12s 463us/step - loss: 1.3824 - categorical_accuracy: 0.7155 - val_loss: 2.0345 - val_categorical_accuracy: 0.6494\n",
      "Epoch 39/100\n",
      "25026/25026 [==============================] - 13s 500us/step - loss: 1.3534 - categorical_accuracy: 0.7203 - val_loss: 2.0142 - val_categorical_accuracy: 0.6535\n",
      "Epoch 40/100\n",
      "25026/25026 [==============================] - 12s 482us/step - loss: 1.3248 - categorical_accuracy: 0.7248 - val_loss: 1.9951 - val_categorical_accuracy: 0.6529\n",
      "Epoch 41/100\n",
      "25026/25026 [==============================] - 12s 465us/step - loss: 1.3006 - categorical_accuracy: 0.7310 - val_loss: 1.9895 - val_categorical_accuracy: 0.6573\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25026/25026 [==============================] - 12s 465us/step - loss: 1.2754 - categorical_accuracy: 0.7354 - val_loss: 1.9697 - val_categorical_accuracy: 0.6620\n",
      "Epoch 43/100\n",
      "25026/25026 [==============================] - 12s 462us/step - loss: 1.2503 - categorical_accuracy: 0.7394 - val_loss: 1.9487 - val_categorical_accuracy: 0.6620\n",
      "Epoch 44/100\n",
      "25026/25026 [==============================] - 12s 485us/step - loss: 1.2265 - categorical_accuracy: 0.7427 - val_loss: 1.9330 - val_categorical_accuracy: 0.6706\n",
      "Epoch 45/100\n",
      "25026/25026 [==============================] - 12s 476us/step - loss: 1.2047 - categorical_accuracy: 0.7475 - val_loss: 1.9265 - val_categorical_accuracy: 0.6692\n",
      "Epoch 46/100\n",
      "25026/25026 [==============================] - 12s 468us/step - loss: 1.1857 - categorical_accuracy: 0.7501 - val_loss: 1.9136 - val_categorical_accuracy: 0.6783\n",
      "Epoch 47/100\n",
      "25026/25026 [==============================] - 11s 445us/step - loss: 1.1655 - categorical_accuracy: 0.7529 - val_loss: 1.8996 - val_categorical_accuracy: 0.6783\n",
      "Epoch 48/100\n",
      "25026/25026 [==============================] - 11s 450us/step - loss: 1.1449 - categorical_accuracy: 0.7590 - val_loss: 1.8819 - val_categorical_accuracy: 0.6831\n",
      "Epoch 49/100\n",
      "25026/25026 [==============================] - 12s 490us/step - loss: 1.1256 - categorical_accuracy: 0.7606 - val_loss: 1.8713 - val_categorical_accuracy: 0.6877\n",
      "Epoch 50/100\n",
      "25026/25026 [==============================] - 12s 487us/step - loss: 1.1095 - categorical_accuracy: 0.7626 - val_loss: 1.8717 - val_categorical_accuracy: 0.6832\n",
      "Epoch 51/100\n",
      "25026/25026 [==============================] - 12s 498us/step - loss: 1.0890 - categorical_accuracy: 0.7685 - val_loss: 1.8533 - val_categorical_accuracy: 0.6917\n",
      "Epoch 52/100\n",
      "25026/25026 [==============================] - 11s 446us/step - loss: 1.0760 - categorical_accuracy: 0.7687 - val_loss: 1.8455 - val_categorical_accuracy: 0.6967\n",
      "Epoch 53/100\n",
      "25026/25026 [==============================] - 11s 432us/step - loss: 1.0545 - categorical_accuracy: 0.7742 - val_loss: 1.8326 - val_categorical_accuracy: 0.6962\n",
      "Epoch 54/100\n",
      "25026/25026 [==============================] - 11s 428us/step - loss: 1.0401 - categorical_accuracy: 0.7768 - val_loss: 1.8322 - val_categorical_accuracy: 0.6973\n",
      "Epoch 55/100\n",
      "25026/25026 [==============================] - 11s 457us/step - loss: 1.0275 - categorical_accuracy: 0.7793 - val_loss: 1.8202 - val_categorical_accuracy: 0.6987\n",
      "Epoch 56/100\n",
      "25026/25026 [==============================] - 10s 395us/step - loss: 1.0090 - categorical_accuracy: 0.7839 - val_loss: 1.8074 - val_categorical_accuracy: 0.7082\n",
      "Epoch 57/100\n",
      "25026/25026 [==============================] - 14s 544us/step - loss: 0.9951 - categorical_accuracy: 0.7855 - val_loss: 1.8110 - val_categorical_accuracy: 0.7058\n",
      "Epoch 58/100\n",
      "25026/25026 [==============================] - 10s 406us/step - loss: 0.9813 - categorical_accuracy: 0.7883 - val_loss: 1.8039 - val_categorical_accuracy: 0.7054\n",
      "Epoch 59/100\n",
      "25026/25026 [==============================] - 10s 405us/step - loss: 0.9664 - categorical_accuracy: 0.7919 - val_loss: 1.7917 - val_categorical_accuracy: 0.7131\n",
      "Epoch 60/100\n",
      "25026/25026 [==============================] - 12s 468us/step - loss: 0.9512 - categorical_accuracy: 0.7945 - val_loss: 1.7868 - val_categorical_accuracy: 0.7139\n",
      "Epoch 61/100\n",
      "25026/25026 [==============================] - 11s 455us/step - loss: 0.9385 - categorical_accuracy: 0.7967 - val_loss: 1.7719 - val_categorical_accuracy: 0.7174\n",
      "Epoch 62/100\n",
      "25026/25026 [==============================] - 11s 456us/step - loss: 0.9291 - categorical_accuracy: 0.7976 - val_loss: 1.7827 - val_categorical_accuracy: 0.7141\n",
      "Epoch 63/100\n",
      "25026/25026 [==============================] - 12s 480us/step - loss: 0.9130 - categorical_accuracy: 0.8013 - val_loss: 1.7636 - val_categorical_accuracy: 0.7165\n",
      "Epoch 64/100\n",
      "25026/25026 [==============================] - 12s 470us/step - loss: 0.9045 - categorical_accuracy: 0.8014 - val_loss: 1.7637 - val_categorical_accuracy: 0.7219\n",
      "Epoch 65/100\n",
      "25026/25026 [==============================] - 12s 460us/step - loss: 0.8917 - categorical_accuracy: 0.8052 - val_loss: 1.7537 - val_categorical_accuracy: 0.7230\n",
      "Epoch 66/100\n",
      "25026/25026 [==============================] - 12s 468us/step - loss: 0.8787 - categorical_accuracy: 0.8077 - val_loss: 1.7556 - val_categorical_accuracy: 0.7280\n",
      "Epoch 67/100\n",
      "25026/25026 [==============================] - 11s 459us/step - loss: 0.8676 - categorical_accuracy: 0.8100 - val_loss: 1.7449 - val_categorical_accuracy: 0.7262\n",
      "Epoch 68/100\n",
      "25026/25026 [==============================] - 12s 462us/step - loss: 0.8566 - categorical_accuracy: 0.8114 - val_loss: 1.7435 - val_categorical_accuracy: 0.7320\n",
      "Epoch 69/100\n",
      "25026/25026 [==============================] - 12s 460us/step - loss: 0.8481 - categorical_accuracy: 0.8125 - val_loss: 1.7392 - val_categorical_accuracy: 0.7310\n",
      "Epoch 70/100\n",
      "25026/25026 [==============================] - 11s 456us/step - loss: 0.8362 - categorical_accuracy: 0.8161 - val_loss: 1.7400 - val_categorical_accuracy: 0.7310\n",
      "Epoch 71/100\n",
      "25026/25026 [==============================] - 12s 478us/step - loss: 0.8273 - categorical_accuracy: 0.8176 - val_loss: 1.7240 - val_categorical_accuracy: 0.7331\n",
      "Epoch 72/100\n",
      "25026/25026 [==============================] - 12s 472us/step - loss: 0.8179 - categorical_accuracy: 0.8193 - val_loss: 1.7187 - val_categorical_accuracy: 0.7345\n",
      "Epoch 73/100\n",
      "25026/25026 [==============================] - 11s 456us/step - loss: 0.8060 - categorical_accuracy: 0.8209 - val_loss: 1.7255 - val_categorical_accuracy: 0.7333\n",
      "Epoch 74/100\n",
      "25026/25026 [==============================] - 10s 407us/step - loss: 0.7987 - categorical_accuracy: 0.8223 - val_loss: 1.7160 - val_categorical_accuracy: 0.7345\n",
      "Epoch 75/100\n",
      "25026/25026 [==============================] - 8s 319us/step - loss: 0.7941 - categorical_accuracy: 0.8230 - val_loss: 1.7116 - val_categorical_accuracy: 0.7369\n",
      "Epoch 76/100\n",
      "25026/25026 [==============================] - 10s 417us/step - loss: 0.7795 - categorical_accuracy: 0.8266 - val_loss: 1.7032 - val_categorical_accuracy: 0.7400\n",
      "Epoch 77/100\n",
      "25026/25026 [==============================] - 11s 447us/step - loss: 0.7666 - categorical_accuracy: 0.8275 - val_loss: 1.6990 - val_categorical_accuracy: 0.7425\n",
      "Epoch 78/100\n",
      "25026/25026 [==============================] - 11s 450us/step - loss: 0.7620 - categorical_accuracy: 0.8273 - val_loss: 1.7019 - val_categorical_accuracy: 0.7393\n",
      "Epoch 79/100\n",
      "25026/25026 [==============================] - 11s 445us/step - loss: 0.7529 - categorical_accuracy: 0.8323 - val_loss: 1.7002 - val_categorical_accuracy: 0.7420\n",
      "Epoch 80/100\n",
      "25026/25026 [==============================] - 12s 497us/step - loss: 0.7475 - categorical_accuracy: 0.8333 - val_loss: 1.6925 - val_categorical_accuracy: 0.7470\n",
      "Epoch 81/100\n",
      "25026/25026 [==============================] - 12s 470us/step - loss: 0.7373 - categorical_accuracy: 0.8326 - val_loss: 1.6865 - val_categorical_accuracy: 0.7449\n",
      "Epoch 82/100\n",
      "25026/25026 [==============================] - 12s 497us/step - loss: 0.7302 - categorical_accuracy: 0.8339 - val_loss: 1.6911 - val_categorical_accuracy: 0.7457\n",
      "Epoch 83/100\n",
      "25026/25026 [==============================] - 12s 497us/step - loss: 0.7205 - categorical_accuracy: 0.8374 - val_loss: 1.6889 - val_categorical_accuracy: 0.7464\n",
      "Epoch 84/100\n",
      "25026/25026 [==============================] - 13s 509us/step - loss: 0.7177 - categorical_accuracy: 0.8384 - val_loss: 1.6879 - val_categorical_accuracy: 0.7467\n",
      "Epoch 85/100\n",
      "25026/25026 [==============================] - 11s 458us/step - loss: 0.7080 - categorical_accuracy: 0.8395 - val_loss: 1.6829 - val_categorical_accuracy: 0.7515\n",
      "Epoch 86/100\n",
      "25026/25026 [==============================] - 11s 445us/step - loss: 0.7049 - categorical_accuracy: 0.8397 - val_loss: 1.6914 - val_categorical_accuracy: 0.7470\n",
      "Epoch 87/100\n",
      "25026/25026 [==============================] - 18s 724us/step - loss: 0.6947 - categorical_accuracy: 0.8399 - val_loss: 1.6869 - val_categorical_accuracy: 0.7496\n",
      "Epoch 88/100\n",
      "25026/25026 [==============================] - 12s 492us/step - loss: 0.6899 - categorical_accuracy: 0.8422 - val_loss: 1.6846 - val_categorical_accuracy: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "25026/25026 [==============================] - 16s 642us/step - loss: 0.6829 - categorical_accuracy: 0.8438 - val_loss: 1.6747 - val_categorical_accuracy: 0.7516\n",
      "Epoch 90/100\n",
      "25026/25026 [==============================] - 13s 526us/step - loss: 0.6712 - categorical_accuracy: 0.8454 - val_loss: 1.6744 - val_categorical_accuracy: 0.7564\n",
      "Epoch 91/100\n",
      "25026/25026 [==============================] - 12s 486us/step - loss: 0.6704 - categorical_accuracy: 0.8463 - val_loss: 1.6727 - val_categorical_accuracy: 0.7548\n",
      "Epoch 92/100\n",
      "25026/25026 [==============================] - 12s 481us/step - loss: 0.6652 - categorical_accuracy: 0.8468 - val_loss: 1.6532 - val_categorical_accuracy: 0.7590\n",
      "Epoch 93/100\n",
      "25026/25026 [==============================] - 13s 515us/step - loss: 0.6536 - categorical_accuracy: 0.8488 - val_loss: 1.6588 - val_categorical_accuracy: 0.7588\n",
      "Epoch 94/100\n",
      "25026/25026 [==============================] - 12s 460us/step - loss: 0.6477 - categorical_accuracy: 0.8503 - val_loss: 1.6540 - val_categorical_accuracy: 0.7577\n",
      "Epoch 95/100\n",
      "25026/25026 [==============================] - 12s 497us/step - loss: 0.6450 - categorical_accuracy: 0.8523 - val_loss: 1.6501 - val_categorical_accuracy: 0.7606\n",
      "Epoch 96/100\n",
      "25026/25026 [==============================] - 12s 464us/step - loss: 0.6393 - categorical_accuracy: 0.8521 - val_loss: 1.6650 - val_categorical_accuracy: 0.7582\n",
      "Epoch 97/100\n",
      "25026/25026 [==============================] - 12s 489us/step - loss: 0.6406 - categorical_accuracy: 0.8519 - val_loss: 1.6543 - val_categorical_accuracy: 0.7633\n",
      "Epoch 98/100\n",
      "25026/25026 [==============================] - 11s 448us/step - loss: 0.6259 - categorical_accuracy: 0.8548 - val_loss: 1.6567 - val_categorical_accuracy: 0.7633\n",
      "Epoch 99/100\n",
      "25026/25026 [==============================] - 10s 382us/step - loss: 0.6243 - categorical_accuracy: 0.8545 - val_loss: 1.6447 - val_categorical_accuracy: 0.7649\n",
      "Epoch 100/100\n",
      "25026/25026 [==============================] - 12s 476us/step - loss: 0.6179 - categorical_accuracy: 0.8558 - val_loss: 1.6559 - val_categorical_accuracy: 0.7636\n"
     ]
    }
   ],
   "source": [
    "model = getmodel()\n",
    "history = model.fit(x_train_partial,\n",
    "          y_train_partial,\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = [index:word for word,index in tokenizer.word_index]\n",
    "def predict(input_lines):\n",
    "    input_lines = [line.strip().lower() + ' end' for line in input_lines]\n",
    "    lines = tokenizer.texts_to_sequences(input_lines)\n",
    "    input_x,input_y = partitionlines(lines,maxlen)\n",
    "    model_input_x = sequence.pad_sequences(input_x,maxlen=maxlen)\n",
    "    output_y = model.predict(model_input_x)\n",
    "    output_seq = np.argmax(output_y,axis = 0)\n",
    "    output_lines = [[reverse_word_index[num] for num in inp] for inp in input_x]\n",
    "    predicted_words = [reverse_word_index[num] for num in output_seq]\n",
    "    expected_words = [reverse_word_index[num] for num in input_y]\n",
    "    for i in range(0,len(input_lines)):\n",
    "        print (\"input = :{0}, predicted = {1},expected = {2}\", output_lines[i],predicted_words[i],expected_words[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = {index:word for word,index in tokenizer.word_index.items()}\n",
    "def wordfromonehot(onehot):\n",
    "    index = np.argmax(onehot)\n",
    "    return reverse_word_index[index]\n",
    "def predict(input_line, count ):\n",
    "    input_line_clean = input_line.strip().lower()\n",
    "    #print(input_lines)\n",
    "    predicted_words = []\n",
    "    input_x= tokenizer.texts_to_sequences([input_line_clean])[0]\n",
    "    for _ in range(0,count):\n",
    "        model_input_x = sequence.pad_sequences([input_x],maxlen=maxlen)\n",
    "        output_y = model.predict(model_input_x)\n",
    "        #print (np.max(output_y))\n",
    "        input_x.append(np.argmax(output_y))\n",
    "        predicted_words.append((wordfromonehot(output_y), np.max(output_y)))\n",
    "    #print(predicted_words)\n",
    "    print (\"input = {0}, predicted = {1}\".format(input_line,predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = Can you please, predicted = [('check', 0.23085266), ('the', 0.35568327), ('quality', 0.8381603), ('and', 0.9865416), ('let', 0.96286523), ('me', 0.9975465), ('know', 0.99354774), ('your', 0.7261728), ('feedback', 0.34738642), ('and', 0.851428)]\n"
     ]
    }
   ],
   "source": [
    "predict(\"Can you please\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 94us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7531049308776856, 0.753]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_words = []\n",
    "for seq in x_test:\n",
    "    x_test_words.append(' '.join([reverse_word_index[num] for num in seq if num != 0 ]))\n",
    "pred_test = model.predict(x_test)\n",
    "\n",
    "y_test_pred_words = [wordfromonehot(pred) for pred in pred_test]\n",
    "y_act = [wordfromonehot(y) for y in y_test]\n",
    "for i in range(100,110):\n",
    "    #print('input = {0}, pred  = {1}, actual = {2}'.format(x_test_words[i],y_test_pred_words[i],y_act[i]))\n",
    "    predict(x_test_words[i],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('outlook_pred.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
